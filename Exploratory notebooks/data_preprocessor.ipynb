{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586b928b2ef449b5",
   "metadata": {},
   "source": [
    "# Data Preprocessing Module MVP for Exploration\n",
    "\n",
    "In this module I will be primarily focusing on the basics of preprocessing textual based data.\n",
    "- Text Cleaning and Normalization\n",
    "- Tokenization\n",
    "- Deduplication\n",
    "- Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "id": "85c5659532873afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:47.389979Z",
     "start_time": "2025-09-15T16:46:43.038719Z"
    }
   },
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Used for normalization and text cleaning\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# For tokenizing\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoTokenizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahul/miniconda3/envs/test_env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt_tab to /Users/rahul/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ba218a83523211e5",
   "metadata": {},
   "source": [
    "### Text Cleaning and normalization\n",
    "This step insures consistency in text input, reducing noise that can adversely affectlater preprocessing or model traning.\n",
    "\n",
    "using the opensource [wikitext-103-raw-v1 dataset from huggingface](https://huggingface.co/datasets/iohadrubin/wikitext-103-raw-v1)"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:50.268384Z",
     "start_time": "2025-09-15T16:46:47.411872Z"
    }
   },
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\"iohadrubin/wikitext-103-raw-v1\", split=\"train\")\n",
    "df_wiki = pd.DataFrame(dataset)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "ae75f43b02ae78f1",
   "metadata": {},
   "source": [
    "Because the size of the preprocessed dataset was too large, I am capping the dataset size to 25 GB for initial analysis"
   ]
  },
  {
   "cell_type": "code",
   "id": "60d8cd045f45bd05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:53.682921Z",
     "start_time": "2025-09-15T16:46:53.450978Z"
    }
   },
   "source": [
    "# Calculate the byte size of each text entry\n",
    "df_wiki['text_size'] = df_wiki['text'].apply(lambda x: len(x.encode('utf-8')))\n",
    "\n",
    "# Compute cumulative size (in bytes)\n",
    "df_wiki['cum_size'] = df_wiki['text_size'].cumsum()\n",
    "\n",
    "# Define maximum allowed size: 25GB in bytes\n",
    "max_bytes = 25 * 1024 * 1024 * 1024\n",
    "\n",
    "# Filter the DataFrame to keep rows until we reach the cap\n",
    "df_wiki_capped = df_wiki[df_wiki['cum_size'] <= max_bytes].copy()\n",
    "print(f\"Original dataset rows: {len(df_wiki)}\")\n",
    "print(f\"Rows retained (capped to ~25GB): {len(df_wiki_capped)}\")\n",
    "\n",
    "# Drop the temporary columns\n",
    "df_wiki_capped.drop(columns=['text_size', 'cum_size'], inplace=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset rows: 29567\n",
      "Rows retained (capped to ~25GB): 29567\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "40cc69a66e3d823f",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "58c7a8419645bb24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:53.763789Z",
     "start_time": "2025-09-15T16:46:53.758194Z"
    }
   },
   "source": [
    "df_wiki_capped.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text\n",
       "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...\n",
       "1  = Tower Building of the Little Rock Arsenal =\\...\n",
       "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...\n",
       "3  = Gambia women's national football team =\\nThe...\n",
       "4  = Plain maskray =\\nThe plain maskray or brown ..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Valkyria Chronicles III =\\nSenjō no Valkyria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal =\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= Cicely Mary Barker =\\nCicely Mary Barker (28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>= Gambia women's national football team =\\nThe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= Plain maskray =\\nThe plain maskray or brown ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "29d842394dca08e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:53.968168Z",
     "start_time": "2025-09-15T16:46:53.963482Z"
    }
   },
   "source": [
    "df_wiki_capped.tail()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                    text\n",
       "29562  = Si Una Vez =\\n\"Si Una Vez\" (English: If I On...\n",
       "29563  = Sicklefin lemon shark =\\nThe sicklefin lemon...\n",
       "29564  = Flammulated flycatcher =\\nThe flammulated fl...\n",
       "29565  = Ontario Highway 89 =\\nKing's Highway 89, com...\n",
       "29566  = Luke Smith (writer) =\\nLuke Michael Smith is..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29562</th>\n",
       "      <td>= Si Una Vez =\\n\"Si Una Vez\" (English: If I On...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29563</th>\n",
       "      <td>= Sicklefin lemon shark =\\nThe sicklefin lemon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29564</th>\n",
       "      <td>= Flammulated flycatcher =\\nThe flammulated fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29565</th>\n",
       "      <td>= Ontario Highway 89 =\\nKing's Highway 89, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29566</th>\n",
       "      <td>= Luke Smith (writer) =\\nLuke Michael Smith is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "a15e684eba5f5845",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:54.191671Z",
     "start_time": "2025-09-15T16:46:54.188769Z"
    }
   },
   "source": [
    "# Basic info\n",
    "print(\"Dataset columns:\", df_wiki_capped.columns)\n",
    "print(\"Dataset shape:\", df_wiki_capped.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['text'], dtype='object')\n",
      "Dataset shape: (29567, 1)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "73f6a5bba99f51d8",
   "metadata": {},
   "source": [
    "#### Data cleaning exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "8e5e4b85a0259e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:54.454981Z",
     "start_time": "2025-09-15T16:46:54.437504Z"
    }
   },
   "source": [
    "# text length for each entry\n",
    "df_wiki_capped['text_length'] = df_wiki_capped['text'].apply(lambda x: len(x))\n",
    "print(\"Text lenght stats:\")\n",
    "print(df_wiki_capped['text_length'].describe())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lenght stats:\n",
      "count     29567.000000\n",
      "mean      17537.078161\n",
      "std       14555.364685\n",
      "min          16.000000\n",
      "25%        7750.000000\n",
      "50%       12994.000000\n",
      "75%       22721.500000\n",
      "max      140098.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3e1f145624cd63e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:46:55.170502Z",
     "start_time": "2025-09-15T16:46:54.587654Z"
    }
   },
   "source": [
    "# Lines with problematic formatting\n",
    "problematic = df_wiki_capped[df_wiki_capped['text'].str.contains(r'[\"]{1,}', na=False)]\n",
    "print(\"Rows with potential quotation issues: \") # decided to do this because of an table formatting error while doing df_wiki.head() step in the IDE.\n",
    "# The error was: “Unterminated quoted field at end of CSV line”\n",
    "print(problematic.head(5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with potential quotation issues: \n",
      "                                                text  text_length\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297\n",
      "1  = Tower Building of the Little Rock Arsenal =\\...        20770\n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...        15371\n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...         6695\n",
      "5  = 2011 – 12 Columbus Blue Jackets season =\\nTh...        17189\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Contamination Simulator\n",
    "\n",
    "This will artificially feed the contaminated data into the wiki-dataset for a artificial contamination simulation"
   ],
   "id": "d8300113a1e38a04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:48:25.647100Z",
     "start_time": "2025-09-15T16:46:55.244697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load irrelevant text dataset\n",
    "gutenberg_ds = load_dataset(\"sedthh/gutenberg_english\", split=\"train\")\n",
    "gutenberg_sentences = gutenberg_ds['TEXT']"
   ],
   "id": "38d74974",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:48:27.005457Z",
     "start_time": "2025-09-15T16:48:26.925913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Text contamination functions\n",
    "def swap_words(text):\n",
    "    words = text.split()\n",
    "    if len(words) < 2:\n",
    "        return text\n",
    "    i, j = random.sample(range(len(words)), 2)\n",
    "    words[i], words[j] = words[j], words[i]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def add_char_noise(text, noise_level=0.05):\n",
    "    def insert(s): return s[:i] + random.choice('abcdefghijklmnopqrstuvwxyz') + s[i:]\n",
    "    def delete(s): return s[:i] + s[i+1:] if len(s) > 1 else s\n",
    "    def substitute(s): return s[:i] + random.choice('abcdefghijklmnopqrstuvwxyz') + s[i+1:]\n",
    "    def swap(s): return s[:i] + s[i+1] + s[i] + s[i+2:] if i < len(s)-1 else s\n",
    "    ops = [insert, delete, substitute, swap]\n",
    "    words = text.split()\n",
    "    for w in range(len(words)):\n",
    "        if random.random() < noise_level and len(words[w]) > 0:\n",
    "            i = random.randint(0, len(words[w])-1)\n",
    "            words[w] = random.choice(ops)(words[w])\n",
    "    return ' '.join(words)\n",
    "\n",
    "def insert_irrelevant_text(text):\n",
    "    irrelevant = random.choice(gutenberg_sentences)\n",
    "    words = text.split()\n",
    "    insert_pos = random.randint(0, len(words))\n",
    "    return ' '.join(words[:insert_pos] + irrelevant.split() + words[insert_pos:])\n",
    "\n",
    "def contaminate_text(text):\n",
    "    text = swap_words(text)\n",
    "    text = add_char_noise(text)\n",
    "    text = insert_irrelevant_text(text)\n",
    "    return text"
   ],
   "id": "815d3d6699da1b88",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:48:59.336244Z",
     "start_time": "2025-09-15T16:48:27.096263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply contamination to 20% of rows\n",
    "contam_indices = df_wiki_capped.sample(frac=0.12, random_state=42).index\n",
    "df_wiki_capped.loc[contam_indices, 'text'] = df_wiki_capped.loc[contam_indices, 'text'].apply(contaminate_text)\n",
    "\n",
    "print(f\"Contaminated {len(contam_indices)} rows using swap, noise, and Gutenberg insert.\")"
   ],
   "id": "d3159b8bee87fc01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contaminated 3548 rows using swap, noise, and Gutenberg insert.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:49:02.792705Z",
     "start_time": "2025-09-15T16:48:59.383726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# text length for each entry\n",
    "df_wiki_capped['text_length'] = df_wiki_capped['text'].apply(lambda x: len(x))\n",
    "print(\"Text lenght stats:\")\n",
    "print(df_wiki_capped['text_length'].describe())"
   ],
   "id": "e11414d52120d57f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text lenght stats:\n",
      "count    2.956700e+04\n",
      "mean     5.877762e+04\n",
      "std      1.660840e+05\n",
      "min      1.600000e+01\n",
      "25%      8.355500e+03\n",
      "50%      1.492900e+04\n",
      "75%      3.039400e+04\n",
      "max      4.892107e+06\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:49:04.550561Z",
     "start_time": "2025-09-15T16:49:02.815936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Lines with problematic formatting\n",
    "problematic = df_wiki_capped[df_wiki_capped['text'].str.contains(r'[\"]{1,}', na=False)]\n",
    "print(\"Rows with potential quotation issues: \") # decided to do this because of an table formatting error while doing df_wiki.head() step in the IDE.\n",
    "# The error was: “Unterminated quoted field at end of CSV line”\n",
    "print(problematic.head(5))"
   ],
   "id": "804d786b7bd4d9f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with potential quotation issues: \n",
      "                                                text  text_length\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...        20297\n",
      "1  = Tower Building of the Little Rock Arsenal =\\...        20770\n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...        15371\n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...         6695\n",
      "5  = 2011 – 12 Columbus Blue Jackets season =\\nTh...        17189\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "6f9ccbd79adfc944",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization\n",
    "cleaning function to:\n",
    "- Remove HTML tags\n",
    "- Normalize Unicode to standardize characters.\n",
    "- Convert text to lowercase and remove accent marks\n",
    "- Remove non-UTF characters and extra whitespaces.\n",
    "\n",
    "This should make the text more uniform for intial tokenization."
   ]
  },
  {
   "cell_type": "code",
   "id": "e94086e91b468bdd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:49:04.571719Z",
     "start_time": "2025-09-15T16:49:04.566949Z"
    }
   },
   "source": [
    "def normalize_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    # Normalize Unicode (NFC)\n",
    "    text = unicodedata.normalize('NFC', text)\n",
    "    # Lowercase conversion and accent stripping\n",
    "    text = text.lower()\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "    # Remove non-UTF characters and extra whitespace\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "8e2f052097384a1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:50:43.669096Z",
     "start_time": "2025-09-15T16:49:04.576084Z"
    }
   },
   "source": [
    "# Applying normalizations\n",
    "df_wiki_capped['cleaned_text'] = df_wiki_capped['text'].apply(normalize_text)\n",
    "print(\"cleaned sample:\")\n",
    "print(df_wiki_capped[['text', 'cleaned_text']].head(5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned sample:\n",
      "                                                text  \\\n",
      "0  = Valkyria Chronicles III =\\nSenjō no Valkyria...   \n",
      "1  = Tower Building of the Little Rock Arsenal =\\...   \n",
      "2  = Cicely Mary Barker =\\nCicely Mary Barker (28...   \n",
      "3  = Gambia women's national football team =\\nThe...   \n",
      "4  = Plain maskray =\\nThe plain maskray or brown ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "1  = tower building of the little rock arsenal = ...  \n",
      "2  = cicely mary barker = cicely mary barker (28 ...  \n",
      "3  = gambia women's national football team = the ...  \n",
      "4  = plain maskray = the plain maskray or brown s...  \n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "ea545470ae7e304c",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenizing the cleaned data using Hugging Face fast tokenizer for essential downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4110d505c06390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:58:17.786283Z",
     "start_time": "2025-09-15T16:50:43.705372Z"
    }
   },
   "source": [
    "# Tokenizer initialization\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2', use_fast=True)\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = tokenizer.tokenize(text, truncation=True, max_length=1024) # changes made because of warning\n",
    "    return tokens\n",
    "\n",
    "df_wiki_capped['tokens'] = df_wiki_capped['cleaned_text'].apply(tokenize_text)\n",
    "print(\"tokenized sample:\")\n",
    "print(df_wiki_capped[['cleaned_text', 'tokens']].head(2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "1  = tower building of the little rock arsenal = ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [=, Ġv, alky, ria, Ġchron, icles, Ġiii, Ġ=, Ġs...  \n",
      "1  [=, Ġtower, Ġbuilding, Ġof, Ġthe, Ġlittle, Ġro...  \n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "90b1e93a81f633e",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "Deduplication is the process of removing duplicates or near-duplicates to avoid redundancy in the dataset. For now I am choosing to remove the exact-matches."
   ]
  },
  {
   "cell_type": "code",
   "id": "319c48482dcdfe24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:58:18.544184Z",
     "start_time": "2025-09-15T16:58:17.833659Z"
    }
   },
   "source": [
    "def duplicate_texts(df, text_column = 'cleaned_text'):\n",
    "    df = df.drop_duplicates(subset=[text_column])\n",
    "    return df\n",
    "\n",
    "df_wiki_capped_unique = duplicate_texts(df_wiki_capped)\n",
    "print(\"Number of rows after deduplication:\", len(df_wiki_capped_unique))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after deduplication: 29224\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "1892177465583057",
   "metadata": {},
   "source": [
    "### Data Segmentation\n",
    "segmenting the cleaned text into sentences for finer analysis using NLTK's sentence tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "id": "6aefa5ac5b9af218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:58:18.553574Z",
     "start_time": "2025-09-15T16:58:18.550895Z"
    }
   },
   "source": [
    "def segment_text(text, mode='sentence', fixed_token_length=100):\n",
    "    if mode == 'sentence':\n",
    "        segments = sent_tokenize(text)\n",
    "    elif mode == 'fixed':\n",
    "        tokens = tokenize_text(text)\n",
    "        segments = [' '.join(tokens[i:i+fixed_token_length]) for i in range(0, len(tokens), fixed_token_length)]\n",
    "    else:\n",
    "        segments = [text]\n",
    "    return segments"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "4a5e5774968bdb35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:58:18.560111Z",
     "start_time": "2025-09-15T16:58:18.556913Z"
    }
   },
   "source": [
    "def segment_dataframe(df, text_column='cleaned_text', mode='sentence'):\n",
    "    df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n",
    "    df_segmented = df.explode('segments')\n",
    "    return df_segmented"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "9298c2ac5db48788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:48.380179Z",
     "start_time": "2025-09-15T16:58:18.565167Z"
    }
   },
   "source": [
    "df_segmented = segment_dataframe(df_wiki_capped_unique, text_column='cleaned_text', mode='sentence')\n",
    "print(\"Segmented data sample:\")\n",
    "print(df_segmented[['cleaned_text', 'segments']].head(5))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3j/w0qrc74d6sj7fjdwc593xcqc0000gn/T/ipykernel_45186/2733018757.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['segments'] = df[text_column].apply(lambda x: segment_text(x, mode=mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented data sample:\n",
      "                                        cleaned_text  \\\n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...   \n",
      "\n",
      "                                            segments  \n",
      "0  = valkyria chronicles iii = senj no valkyria 3...  \n",
      "0  valkyria of the battlefield 3), commonly refer...  \n",
      "0  released in january 2011 in japan, it is the t...  \n",
      "0  employing the same fusion of tactical and real...  \n",
      "0  the game began development in 2010, carrying o...  \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "66e8bc80391c8886",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:48.443475Z",
     "start_time": "2025-09-15T16:59:48.435271Z"
    }
   },
   "source": [
    "len(df_segmented)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14938261"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:48.495953Z",
     "start_time": "2025-09-15T16:59:48.451296Z"
    }
   },
   "cell_type": "code",
   "source": "df_segmented.info()",
   "id": "9bcb55471482f671",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14938261 entries, 0 to 29566\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Dtype \n",
      "---  ------        ----- \n",
      " 0   text          object\n",
      " 1   text_length   int64 \n",
      " 2   cleaned_text  object\n",
      " 3   tokens        object\n",
      " 4   segments      object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 683.8+ MB\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:49.024703Z",
     "start_time": "2025-09-15T16:59:48.499105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dropping the original text column as we don't need that for further modules\n",
    "df_segmented.drop(columns=['text', 'cleaned_text'], inplace=True)"
   ],
   "id": "bda78494de1cd58e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:49.033003Z",
     "start_time": "2025-09-15T16:59:49.029144Z"
    }
   },
   "cell_type": "code",
   "source": "df_segmented.info()",
   "id": "a21dc47f613e472c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14938261 entries, 0 to 29566\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   text_length  int64 \n",
      " 1   tokens       object\n",
      " 2   segments     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 455.9+ MB\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "f1f2674081e7cee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:49.043879Z",
     "start_time": "2025-09-15T16:59:49.041101Z"
    }
   },
   "source": [
    "# Further capping the data\n",
    "# Calculate total number of segmented rows\n",
    "total_rows = len(df_segmented)\n",
    "one_tenth_rows = int(total_rows / 10)\n",
    "print(f\"Total segmented rows: {total_rows}\")\n",
    "print(f\"Keeping only a quater: {one_tenth_rows} rows\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total segmented rows: 14938261\n",
      "Keeping only a quater: 1493826 rows\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "5e7a0f5e1a28035d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:49.107118Z",
     "start_time": "2025-09-15T16:59:49.048403Z"
    }
   },
   "source": [
    "# Select the first one third of the rows\n",
    "df_segmented_subset = df_segmented.iloc[:one_tenth_rows].copy()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T16:59:49.165837Z",
     "start_time": "2025-09-15T16:59:49.111868Z"
    }
   },
   "cell_type": "code",
   "source": "df_segmented_subset.info()",
   "id": "aa706934de58feca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1493826 entries, 0 to 3047\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count    Dtype \n",
      "---  ------       --------------    ----- \n",
      " 0   text_length  1493826 non-null  int64 \n",
      " 1   tokens       1493826 non-null  object\n",
      " 2   segments     1493826 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 45.6+ MB\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "7ec0e3e70a0131d1",
   "metadata": {},
   "source": [
    "### Saving the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3cb944bb1915ce4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T17:02:35.668229Z",
     "start_time": "2025-09-15T16:59:49.173588Z"
    }
   },
   "source": [
    "output_dir = \"../data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"preprocessed_wikitext103_subset.csv\")\n",
    "\n",
    "# Save the segmented DataFrame to the specified CSV file\n",
    "df_segmented_subset.to_csv(output_file, index=False)\n",
    "print(f\"Subset of preprocessed data saved to {output_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of preprocessed data saved to ../data/preprocessed_wikitext103_subset.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "66f7416d5ca3108d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T17:09:26.392120Z",
     "start_time": "2025-09-15T17:09:11.707345Z"
    }
   },
   "source": [
    "# testing by loading the data\n",
    "\n",
    "# Define the file path in the \"data\" directory\n",
    "data_file = os.path.join(\"../data\", \"preprocessed_wikitext103_subset.csv\")\n",
    "\n",
    "# loading data in chunks as the data size is massive\n",
    "with open(data_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    total_lines = sum(1 for line in f) - 1\n",
    "\n",
    "chunk_size = 10000\n",
    "total_chunks = total_lines // chunk_size\n",
    "\n",
    "# Read CSV in chunks and display progress\n",
    "chunks = []\n",
    "for chunk in tqdm(pd.read_csv(data_file, on_bad_lines='skip', engine='python', chunksize=chunk_size),\n",
    "                  total=total_chunks,\n",
    "                  desc=\"Loading CSV\"):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Combine all chunks into one DataFrame\n",
    "df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Display a sample of the loaded data and its dimensions\n",
    "print(\"Loaded data sample:\")\n",
    "print(df.head())\n",
    "print(\"\\nShape of loaded data:\", df.shape)"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 15\u001B[39m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Read CSV in chunks and display progress\u001B[39;00m\n\u001B[32m     14\u001B[39m chunks = []\n\u001B[32m---> \u001B[39m\u001B[32m15\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon_bad_lines\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mskip\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mpython\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m                  \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtotal_chunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m                  \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mLoading CSV\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[32m     18\u001B[39m     chunks.append(chunk)\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Combine all chunks into one DataFrame\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/test_env/lib/python3.13/site-packages/tqdm/notebook.py:234\u001B[39m, in \u001B[36mtqdm_notebook.__init__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    232\u001B[39m unit_scale = \u001B[32m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.unit_scale \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.unit_scale \u001B[38;5;129;01mor\u001B[39;00m \u001B[32m1\u001B[39m\n\u001B[32m    233\u001B[39m total = \u001B[38;5;28mself\u001B[39m.total * unit_scale \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.total \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.total\n\u001B[32m--> \u001B[39m\u001B[32m234\u001B[39m \u001B[38;5;28mself\u001B[39m.container = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstatus_printer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mncols\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    235\u001B[39m \u001B[38;5;28mself\u001B[39m.container.pbar = proxy(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m    236\u001B[39m \u001B[38;5;28mself\u001B[39m.displayed = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/test_env/lib/python3.13/site-packages/tqdm/notebook.py:108\u001B[39m, in \u001B[36mtqdm_notebook.status_printer\u001B[39m\u001B[34m(_, total, desc, ncols)\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[38;5;66;03m# Fallback to text bar if there's no total\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001B[39;00m\n\u001B[32m    101\u001B[39m \u001B[38;5;66;03m# if not total:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    105\u001B[39m \n\u001B[32m    106\u001B[39m \u001B[38;5;66;03m# Prepare IPython progress bar\u001B[39;00m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m IProgress \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:  \u001B[38;5;66;03m# #187 #451 #558 #872\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(WARN_NOIPYW)\n\u001B[32m    109\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m total:\n\u001B[32m    110\u001B[39m     pbar = IProgress(\u001B[38;5;28mmin\u001B[39m=\u001B[32m0\u001B[39m, \u001B[38;5;28mmax\u001B[39m=total)\n",
      "\u001B[31mImportError\u001B[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T17:02:51.939867Z",
     "start_time": "2025-04-07T19:49:49.820343Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7c67bab78ea22a23",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
